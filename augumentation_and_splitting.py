# -*- coding: utf-8 -*-
"""augumentation_and_splitting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Re4faKXFqLMOA4DFnZnLaT_lLmZz9fGA
"""

from google.colab import drive

drive.mount('/content/drive')

import os
import random
from PIL import Image
from imgaug import augmenters as iaa
import numpy as np
import shutil
import torch
from transformers import ViTImageProcessor

# Paths
original_data_path = "/content/drive/MyDrive/Project/train_images"
augmented_data_path = "/content/drive/MyDrive/Project/augmented_images"
preprocessed_data_path = "/content/drive/MyDrive/Project/preprocessed_data"

# Target number of images per class
target_images_per_class = 1000

# Define augmentation techniques
augmentation_methods = iaa.Sequential([
    iaa.Fliplr(0.5),
    iaa.Affine(rotate=(-30, 30)),
    iaa.Multiply((0.9, 1.1)),
    iaa.AdditiveGaussianNoise(scale=(0, 0.02 * 255)),
    iaa.CropAndPad(percent=(-0.2, 0.2)),
    iaa.LinearContrast((0.8, 1.2))
])

# Function to augment and save images

def augment_and_save(folder_name, target_count):
    input_folder = os.path.join(original_data_path, folder_name)
    output_folder = os.path.join(augmented_data_path, folder_name)
    os.makedirs(output_folder, exist_ok=True)

    images = [f for f in os.listdir(input_folder) if f.endswith(('jpg', 'png', 'jpeg'))]
    current_count = len(images)

    if current_count > target_count:
        # If more images than needed, randomly select target_count images
        selected_images = random.sample(images, target_count)
        print(f"{folder_name}: Found {current_count} images. Reducing to {target_count} images.")
    else:
        # If fewer images, use all existing and augment the deficit
        selected_images = images
        deficit = target_count - current_count
        print(f"{folder_name}: Found {current_count} images. Augmenting {deficit} more.")

    # Save selected original images
    for img_name in selected_images:
        img_path = os.path.join(input_folder, img_name)
        dest_path = os.path.join(output_folder, os.path.splitext(img_name)[0] + ".jpg")
        Image.open(img_path).convert("RGB").save(dest_path, "JPEG")

    # Augment if there is a deficit
    if current_count < target_count:
        for i in range(deficit):
            img_name = random.choice(images)
            img_path = os.path.join(input_folder, img_name)
            image = Image.open(img_path).convert("RGB")
            image_np = np.array(image)

            # Apply augmentations
            augmented_image_np = augmentation_methods(image=image_np)
            augmented_image = Image.fromarray(augmented_image_np)
            augmented_image.save(os.path.join(output_folder, f"aug_{i + 1}.jpg"), "JPEG")

    print(f"Processing completed for {folder_name}. Total images: {target_count}.")

# Augment data for each class
folders = [f for f in os.listdir(original_data_path) if os.path.isdir(os.path.join(original_data_path, f))]
for folder in folders:
    augment_and_save(folder, target_images_per_class)

print(folders)

# Split augmented dataset into train, validation, and test sets
def split_augmented_data(base_dir, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):
    train_dir = os.path.join(base_dir, "train")
    val_dir = os.path.join(base_dir, "val")
    test_dir = os.path.join(base_dir, "test")

    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(val_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    for folder in folders:
        class_dir = os.path.join(augmented_data_path, folder)
        images = [f for f in os.listdir(class_dir) if f.endswith(('jpg', 'png', 'jpeg'))]
        random.shuffle(images)

        train_split = int(len(images) * train_ratio)
        val_split = int(len(images) * (train_ratio + val_ratio))

        train_images = images[:train_split]
        val_images = images[train_split:val_split]
        test_images = images[val_split:]

        # Create class folders
        os.makedirs(os.path.join(train_dir, folder), exist_ok=True)
        os.makedirs(os.path.join(val_dir, folder), exist_ok=True)
        os.makedirs(os.path.join(test_dir, folder), exist_ok=True)

        # Move images
        for img in train_images:
            shutil.copy(os.path.join(class_dir, img), os.path.join(train_dir, folder, img))
        for img in val_images:
            shutil.copy(os.path.join(class_dir, img), os.path.join(val_dir, folder, img))
        for img in test_images:
            shutil.copy(os.path.join(class_dir, img), os.path.join(test_dir, folder, img))

        print(f"{folder}: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test images saved.")

# Execute the split
split_augmented_data("/content/drive/MyDrive/Project/split_data")

# Preprocess and save data for faster loading
image_processor = ViTImageProcessor.from_pretrained("google/vit-large-patch16-224-in21k")

def preprocess_and_save(data_dir, output_file):
    all_images, all_labels = [], []
    class_names = os.listdir(data_dir)

    for label, class_name in enumerate(class_names):
        class_folder = os.path.join(data_dir, class_name)
        for img_file in os.listdir(class_folder):
            img_path = os.path.join(class_folder, img_file)
            image = Image.open(img_path).convert("RGB")
            processed_image = image_processor(images=image, return_tensors="pt")["pixel_values"]
            all_images.append(processed_image)
            all_labels.append(label)

    data = torch.cat(all_images)
    labels = torch.tensor(all_labels)
    torch.save((data, labels), output_file)
    print(f"Saved preprocessed data to {output_file}")

os.makedirs(preprocessed_data_path, exist_ok=True)
preprocess_and_save("/content/drive/MyDrive/Project/split_data/train", os.path.join(preprocessed_data_path, "train_data.pt"))
preprocess_and_save("/content/drive/MyDrive/Project/split_data/val", os.path.join(preprocessed_data_path, "val_data.pt"))
preprocess_and_save("/content/drive/MyDrive/Project/split_data/test", os.path.join(preprocessed_data_path, "test_data.pt"))